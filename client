"""
Corrections pour le client MCP Llama
"""

# 1. Correction du progress_handler (ajouter self)
async def progress_handler(
    self,
    progress: float, 
    total: float | None, 
    message: str | None
) -> None:
    if total is not None:
        percentage = (progress / total) * 100
        self.logger.info(f"Progress: {percentage:.1f}% - {message or ''}")
    else:
        self.logger.info(f"Progress: {progress} - {message or ''}")

# 2. AmÃ©lioration de l'extraction des tool calls
def _extract_tool_call(self, response: str) -> Optional[Dict]:
    """Extract a tool call from the LLM response"""
    import re
    self.logger.debug(f"Parsing for tool call in response")
    
    # Chercher un bloc JSON avec "tool" et "arguments"
    json_pattern = r'\{[^{}]*"tool"[^{}]*"arguments"[^{}]*\}'
    matches = re.finditer(json_pattern, response, re.DOTALL)
    
    for match in matches:
        try:
            json_str = match.group()
            tool_call = json.loads(json_str)
            
            if "tool" in tool_call and "arguments" in tool_call:
                self.logger.info(f"Tool call found: tool={tool_call['tool']}")
                return tool_call
        except json.JSONDecodeError:
            continue
    
    self.logger.debug("No valid tool call found in response")
    return None

# 3. Suppression de _ask_llm (non utilisÃ© correctement)
# Utiliser directement chat() pour toutes les interactions

# 4. AmÃ©lioration de la mÃ©thode chat pour gÃ©rer les erreurs
async def chat(self, user_message: str, max_iterations: int = 5):
    """Handle a conversation with tool calls"""
    if not self.server_config:
        raise ValueError("Server configuration not set. Call set_server_config() first.")
    
    if not self.client:
        self.create_client()

    async with self.client as client:
        # Get available tools list on first connection
        if not self.tools:
            self.logger.info("Fetching tools from server...")
            try:
                self.tools = await client.list_tools()
                print(f"âœ“ Connected to MCP server. {len(self.tools)} tools available:")
                for tool in self.tools:
                    print(f"  - {tool.name}: {tool.description}")
            except Exception as e:
                self.logger.error(f"Failed to list tools: {e}")
                print(f"âš ï¸  Warning: Could not fetch tools from server")
                self.tools = []
        
        # Prepare messages
        messages = [
            {
                "role": "system",
                "content": f"You are a helpful assistant. {self._build_tools_prompt()}"
            }
        ]
        
        # Add history (keep last 10 exchanges to avoid context overflow)
        messages.extend(self.conversation_history[-20:])
        
        # Add new message
        messages.append({
            "role": "user",
            "content": user_message
        })
        
        for iteration in range(max_iterations):
            print(f"\n--- Iteration {iteration + 1} ---")
            
            try:
                # Generate response
                response = self._generate_response(messages)
                
                # Check for tool call
                tool_call = self._extract_tool_call(response)
                
                if tool_call:
                    tool_name = tool_call["tool"]
                    arguments = tool_call["arguments"]
                    
                    print(f"ğŸ”§ Calling tool: {tool_name}")
                    print(f"   Arguments: {json.dumps(arguments, indent=2)}")
                    
                    # Execute tool
                    tool_result = await self._execute_tool_call(client, tool_name, arguments)
                    print(f"   Result: {tool_result[:500]}...")
                    
                    # Add to messages for next iteration
                    messages.append({
                        "role": "assistant",
                        "content": response
                    })
                    messages.append({
                        "role": "user",
                        "content": f"Tool {tool_name} returned: {tool_result}"
                    })
                else:
                    # No tool call, this is the final response
                    print(f"\nğŸ¤– Assistant: {response}")
                    
                    # Save to history
                    self.conversation_history.append({
                        "role": "user",
                        "content": user_message
                    })
                    self.conversation_history.append({
                        "role": "assistant",
                        "content": response
                    })
                    
                    return response
                    
            except Exception as e:
                self.logger.error(f"Error in iteration {iteration + 1}: {e}")
                print(f"âš ï¸  Error: {e}")
                # Continue to next iteration or break if critical
                if iteration == max_iterations - 1:
                    return f"Error occurred: {str(e)}"
        
        return "Maximum iterations reached without final answer"

# 5. AmÃ©lioration du sampling_handler
async def sampling_handler(self,
    messages: list[SamplingMessage],
    params: SamplingParams,
    context: RequestContext
) -> str:
    """Handle sampling requests from the MCP server"""
    self.logger.info("Received sampling request from server")
    
    # Convert MCP messages to our format
    converted_messages = []
    for message in messages:
        content = message.content.text if hasattr(message.content, 'text') else str(message.content)
        converted_messages.append({
            "role": message.role,
            "content": content
        })
    
    # Generate response
    response = self._generate_response(converted_messages, max_new_tokens=params.maxTokens or 512)
    self.logger.info(f"Generated response for server (length: {len(response)})")
    
    return response